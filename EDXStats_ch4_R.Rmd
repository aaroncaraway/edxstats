---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

So we're going to use the direction as a response and
see if we can predict it as a binary response using logistic
regression.

```{r}
require(ISLR)
names(Smarket)
summary(Smarket)
```

```{r}
?Smarket
pairs(Smarket,col=Smarket$Direction)
```

```{r}
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,
            data=Smarket, family=binomial)

summary(glm.fit)
```

```{r}
# This is making predictions on our training data
glm.probs=predict(glm.fit, type="response")
glm.probs[1:5]

# if else, quick assignment
glm.pred=ifelse(glm.probs>0.5, "Up", "Down")
attach(Smarket)
table(glm.pred, Direction)
```

```{r}
mean(glm.pred==Direction)
```

## MAKE TRAINING AND TEST SET

```{r}

train = Year<2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume, 
            data=Smarket, family=binomial, subset=train)
glm.probs=predict(glm.fit, newdata=Smarket[!train,], type="response")
glm.pred=ifelse(glm.probs > 0.5, "Up", "Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
```

## OH NO!! It got WORSE!!

### Let's try a smaller model

```{r}
glm.fit=glm(Direction~Lag1+Lag2,
            data=Smarket, family=binomial, subset=train)
glm.probs=predict(glm.fit, newdata=Smarket[!train,], type="response")
glm.pred=ifelse(glm.probs > 0.5, "Up", "Down")
Direction.2005=Smarket$Direction[!train]
table(glm.pred, Direction.2005)
mean(glm.pred==Direction.2005)
```

### Much better!

# Linear Discriminant Analysis

So the first thing we do, we're going to use the stock
market data here, and the response is going to be the
direction that the market took on a particular day.
And we're going to use the returns on the previous two
days to try and predict the direction on
this particular day.


```{r}
require(ISLR)
require(MASS)
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year<2005)
lda.fit
plot(lda.fit)

Smarket.2005=subset(Smarket,Year==2005)
lda.pred=predict(lda.fit, Smarket.2005)
# lda.pred[1:5,]

class(lda.pred)
```

So the prior probabilities are just the proportions of ups
and downs in the data set.
It's roughly 50%, which says something
about the market, right?
It's kind of a random walk.
Half the time it goes up, half the time it goes down.
It summarizes the group means for the two groups, for the
downs and the ups.
It looks like there may be a slight difference
in these two groups.
The means seem to be separated a little bit.
And then it gives the LDA coefficients.
So if you remember the LDA function fits a linear
function for separating the two groups.
And so, it's got two coefficients.

```{r}
data.frame(lda.pred)[1:5,]
table(lda.pred$class,Smarket.2005$Direction)
mean(lda.pred$class==Smarket.2005$Direction)
```



We'll do a table of that, and we get the little confusion
matrix, which tells us which downs were classified as down,
which downs are classified as up, and so on.
And it's the off-diagonal elements of those that are the
mistakes, and the diagonal elements which are the correct
classifications.
And so, one simple command tells us our current
classification rate, which is we've got a conditional here,
which is the predicted class is equal to the true class.
So that'll be true if they're equal, and it'll be false if
they're not.
And since trues and falses can be coerces to be zeros and
ones, we can just take the mean of that, and it'll give
us our current classification rate, which in this case is
about 0.56.

## K-NEAREST NEIGHBORS!!

k equals 1 says we want one nearest neighbor
classification.
And, again, just to remind you, that means what the
algorithm does is, it says to classify a new observation,
you go into the training set in the x space, the feature
space, and you look for the training observation that's
closest to your test point in Euclidean distance and you
classify to its class.

```{r}
library(class)
?knn
attach(Smarket)
Xlag=cbind(Lag1,Lag2)
train=Year<2005
knn.pred=knn(Xlag[train,], Xlag[!train,], Direction[train], k=1)
table(knn.pred, Direction[!train])
mean(knn.pred==Direction[!train])
```


And it's exactly 0.5.
So it was useless.
One nearest neighbor did no better than flipping a coin.
So, what to do next?
Well, one could proceed further and try nearest
neighbors with multiple values of k.



FIN.