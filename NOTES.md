[Reddit response](https://www.reddit.com/r/learnmachinelearning/comments/fot65q/cant_retain_concepts_unless_they_are_related_to/)

Summary recs:

* Honestly, for me at least, the number one thing that's helped me quickly learn new ML concepts, is heavily investing in the math. I don't expect to ever do the work of a proper mathematician or anything, but it's such a relief to be able to learn new ML stuff so much easier than before. Deep understanding of (at least) linear algebra, calculus, statistics, and whatever other relevant stuff you can stomach learning (analysis, dynamic systems, measure theory) it all helps make learning ML much faster.

* when the choice is 'spend three one hour blocks on 3 sections I'll have 100% forgotten in 3 months' and 'spend 3 hours on one section that I'll retain enough of to remember and apply (possibly with some refresh research) in a year' then I think it's better to move slower, and retain what you're learning.

* I recommend Alcock's 'how to think about analysis' if you're new to proof based math. It'll help ground calc much more intuitively too. Boyd's linear algebra and least squares book is a great crash course in practical linear algebra for data science (along other things).